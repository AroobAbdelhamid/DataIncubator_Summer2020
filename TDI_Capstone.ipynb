{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my project, I am using climate change precipitation changes (as a proxy for flooding/hurricanes) to predict future housing real estate prices in the continental U.S. While climate change is already established, humans have done a poor job so far adapting to the new changes. The future of our economy is dependent on climate change and its effects on the United States geography, including from hurricanes, flooding, and other extreme weather events and it would benefit people greatly if they were aware of the places best suited to live, where new city hubs are likely to be centered in the future and where forward-thinking businesses will begin focusing on now. To do this, I plan to use either Random Forests or Matrix Factorization to distinguish the role of precipitation in determining the cost of an area and use that along with expected future temperature to predict housing prices in the future. By placing these expected costs on an average precipitation map of the US (by season), individuals and families can gain a better understanding of where the future of their life is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#magic getting igor-like graphs\n",
    "#%matplotlib widget\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[] need to match house dates with T_Pr dates\n",
    "[] plot house prices for T_Pr vs zip code\n",
    "[] get sum yearly precipitation vs average yearly house price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to SQLite DB successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlite3.Connection at 0x230b3aa04e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load SQL\n",
    "def create_connection(path):\n",
    "\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = sqlite3.connect(path)\n",
    "        print(\"Connection to SQLite DB successful\")\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "    return connection\n",
    "\n",
    "create_connection(\"C:/Users/Aroob Abdelhamid/earth-analytics/housing.sqlite\") #create a new database first before running this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load city_info first to get station names from ID, and get 'Lon' and 'Lat' and append the loc info to loaded csv with the same filename as ID in city_info\n",
    "def load_temp_precip_files():\n",
    "    path = r'C:\\Users\\Aroob Abdelhamid\\earth-analytics\\DataIncubator\\CMU_TempPrecip_Meas'\n",
    "    #load city_info first\n",
    "    city_info = pd.read_csv(path+\"\\city_info.csv\")\n",
    "    \n",
    "    all_files = []\n",
    "    \n",
    "    #rest of the files   \n",
    "    T_Pr_files = glob.glob(path + \"/US*.csv\")#\"/US*.csv\")\n",
    "    for filename in T_Pr_files:\n",
    "        f = pd.read_csv(filename, header=0)\n",
    "        nm_ext = os.path.basename(filename); nm = os.path.splitext(nm_ext) #get just the filename to search in city_info\n",
    "        \n",
    "        #get city_info_data into precip data\n",
    "        ID_ind = city_info.index[city_info['ID'] == nm[0]]\n",
    "        Lat = city_info['Lat'][ID_ind[0]]; Lon = city_info['Lon'][ID_ind[0]]\n",
    "        f['Lat'] = Lat; f['Lon'] = Lon      \n",
    "        f['Date']= pd.to_datetime(f['Date'], format = '%Y-%m')+MonthEnd(0)\n",
    "        f['year']= f['Date'].dt.year; f['month']= f['Date'].dt.month; f['day']= f['Date'].dt.day\n",
    "        f_avg = f.groupby(['year','month'], sort=False, as_index=False).mean().reset_index()\n",
    "        \n",
    "        test = f_avg[['year', 'month', 'day']]\n",
    "        f_avg['Date']= pd.to_datetime(test, format='%Y-%m-%d') \n",
    "        \n",
    "#       put it all together\n",
    "        all_files.append(f_avg)\n",
    "    \n",
    "    T_Pr = pd.concat(all_files, axis=0, ignore_index=True)\n",
    "    T_Pr = T_Pr.set_index(['Date'])\n",
    "    return T_Pr,f\n",
    "\n",
    "#load_temp_precip_files()\n",
    "T_Pr,f = load_temp_precip_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:/Users/AROOBA~1/AppData/Local/Temp/xpython_8252/94337530.py:2: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  T_Pr_dates = T_Pr.loc['1996':'2020'].reset_index()\n"
     ]
    }
   ],
   "source": [
    "#only keep the dates that zillow has, i.e. after 1996\n",
    "T_Pr_dates = T_Pr.loc['1996':'2020'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(T_Pr['Lon'], T_Pr['Lat'], vmin=0, vmax=0.1 , c= T_Pr['prcp'], cmap='gray')\n",
    "plt.xlabel('longitude')\n",
    "plt.grid(axis='both', alpha=0.5)\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.get_yaxis().labelpad = 15\n",
    "cbar.ax.set_ylabel('Precip (in)', rotation=270)\n",
    "plt.ylabel('latitude')\n",
    "plt.title('Map of Measured Precipitation (in)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the precipitation is highest at the coasts and specifically in the SE. As climate change makes hurricanes more intense and changes precipitation patterns, I expect the areas in yellow to be some of the hardest hit, and that is where I expect the housing market to show the most changes. We can inspect that by comparing the housing market prices to these values and seeing if they changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_house_prices_data():\n",
    "    #load files\n",
    "    path = r'C:\\Users\\Aroob Abdelhamid\\earth-analytics\\DataIncubator'\n",
    "    house = pd.read_csv(path+\"/Zip_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_mon.csv\")\n",
    "\n",
    "    #add zipcode lat/lon\n",
    "    zipcode = pd.read_csv(path+\"/us-zip-code-latitude-and-longitude.csv\")\n",
    "    \n",
    "    ziplist = house['RegionName'].tolist()\n",
    "    house_zip = [[0]*2]*house.shape[0]\n",
    "    for iloc in range(zipcode.shape[0]):\n",
    "        if zipcode['Zip'][iloc] in ziplist:\n",
    "            currzip = zipcode['Zip'][iloc]\n",
    "            zip_ind= house[ziplist == currzip].index.values\n",
    "            house_zip[zip_ind[0]] = zipcode['Latitude'][iloc], zipcode['Longitude'][iloc]      \n",
    "            \n",
    "    house[['Lat', 'Lon']]= pd.DataFrame(house_zip)\n",
    "    return(zipcode, house)   \n",
    "\n",
    "(zipcode,house) = load_house_prices_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_ind_datetime():\n",
    "    house_better = house.T\n",
    "\n",
    "    date_list = ['']*house_better.shape[0]\n",
    "    for idate in range(house_better.shape[0]):\n",
    "         if \"-\" in house_better.index[idate]: #if it's a date\n",
    "                currdate = datetime.strptime(str(house_better.index[idate]), '%Y-%m-%d').date()\n",
    "                date_list[idate] = currdate\n",
    "    house_better['Date'] = pd.to_datetime(date_list)\n",
    "    return house_better\n",
    "\n",
    "house_better = conv_ind_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_T_to_house():\n",
    "    #T_Pr has fewer locations, so use that just to get the unique lat and lon\n",
    "    T_P_latlons =T_Pr_dates.set_index(['Date']).groupby(['Lat','Lon']).size().reset_index().rename(columns = {0: \"num meas\"})\n",
    "    \n",
    "    temppd_house = pd.DataFrame(np.zeros((house_better.shape[0],0)))\n",
    "    temppd_house['Date'] = pd.to_datetime(house_better['Date'].tolist(), format='%Y-%m-%d %f') \n",
    "    \n",
    "    latstouse = house_better.loc['Lat'][0:30343].astype(str).astype(float)\n",
    "    lonstouse = house_better.loc['Lon'][0:30343].astype(str).astype(float)\n",
    "   \n",
    "    final = pd.DataFrame(np.zeros((0, temppd_house.shape[1]+T_Pr_dates.shape[1])), columns = {'Date','Lat','Lon','Price','day','index','level_0', 'month',\n",
    "             'prcp','tmax','tmin','year'})\n",
    "    \n",
    "    for imeas in range(T_P_latlons.shape[0]):\n",
    "        start_time = time.time()\n",
    "        lat = T_P_latlons['Lat'].loc[imeas]\n",
    "        lon = T_P_latlons['Lon'].loc[imeas]  \n",
    "        \n",
    "        if imeas>=1:\n",
    "            prevlat = T_P_latlons['Lat'].loc[imeas-1]\n",
    "            prevlon = T_P_latlons['Lon'].loc[imeas-1]\n",
    "            if abs(prevlat - lat)<1E-4 and abs(prevlon - lon)<1E-4:\n",
    "                continue\n",
    "                \n",
    "        cr1 = T_Pr_dates['Lat']== float(lat)\n",
    "        cr2 = T_Pr_dates['Lon']== float(lon)\n",
    "        temppd_TP = T_Pr_dates[cr1 & cr2].reset_index()\n",
    "        temppd_TP['Date'] = pd.to_datetime(temppd_TP['Date'].astype('str'), format='%Y-%m-%d')\n",
    "            \n",
    "        for ispot in range(house_better.shape[1]-1):\n",
    "            currlat = latstouse[ispot] #float(house_better.loc['Lat'][ispot])\n",
    "            if abs(currlat - lat)<0.03:\n",
    "                #print(ispot)\n",
    "                currlon = float(lonstouse[ispot]) #house_better.loc['Lon'][ispot])\n",
    "                if abs(currlon - lon)<0.03:\n",
    "                #    print(currlat,lat, currlon,lon, imeas, ispot)\n",
    "                    temppd_house[1] = house_better[ispot]            \n",
    "\n",
    "                    merged = pd.merge(left=temppd_house, left_on='Date', right=temppd_TP, right_on='Date')\n",
    "                    final = pd.concat([final, merged], axis=0) \n",
    "                #    print(\"run time for row %s is %s seconds\" % (imeas, time.time() - start_time))\n",
    "                    break          \n",
    "        \n",
    "    return final\n",
    "final = match_T_to_house()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "plt.scatter(house_better.loc['Lon'][0:30342], house_better.loc['Lat'][0:30342])\n",
    "plt.scatter(T_Pr_dates['Lon'],T_Pr_dates['Lat'])\n",
    "plt.ylim([24,50]); plt.xlim([-130, -60]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f, ax = plt.subplots()\n",
    "#plt.scatter(house_better.loc['Lon'][0:20000], house_better.loc['Lat'][0:20000])\n",
    "# plt.scatter(T_P_latlons['Lon'][0:10],T_P_latlons['Lat'][0:10])\n",
    "# plt.ylim([24,50]); plt.xlim([-130, -60]);\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_group = final.groupby(['Lat','Lon'])\n",
    "avg = final_group.mean()\n",
    "std = final_group.std()\n",
    "avg\n",
    "\n",
    "plt.scatter(avg['prcp'],avg['Unnamed: 0'],  vmin=20, vmax=60, c= avg['tmin'])\n",
    "plt.grid(axis='both', alpha=0.5)\n",
    "plt.xlabel('precipitation (in)'); plt.ylabel('Housing Price ($)')\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.get_yaxis().labelpad = 15\n",
    "cbar.ax.set_ylabel('Tmax (F)', rotation=270)\n",
    "plt.title('Housing Prices as a function of Local Precipitation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, I have loaded the precipitation data, and I have loaded the housing data. I plan to use latitude and longitude to connect the housing data to the precipitation data (by city), so I have connected the precipitation data to latitudes and longitudes, as well as averaged the precipitation data by year and month, as the housing data is monthly. \n",
    "Due to time constraints, I have been unable to finish connecting the two different datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
